INTERNAL MODEL PROCESSING: SENTENCETRANSFORMER PIPELINE
Generated: 2025-08-12T05:05:37.519037
================================================================================

STEP 1: INPUT TEXT TO MODEL
--------------------------------------------------
Input text (first 200 chars): {
  "name": "Priya Sharma",
  "education": [
    {
      "school": "Carnegie Mellon University",
      "degree": "master's",
      "major": "Artificial Intelligence",
      "gpa": "3.9"
    },
    {
 ...
Text length: 1635 characters
Text type: Enhanced resume with extracted features

STEP 2: TOKENIZATION PROCESS
--------------------------------------------------
```python
# Input text sample:
text = "{
  "name": "Priya Sharma",
  "education": [
    {
      "sc..."

# ↓ TOKENIZATION (Word-piece/Subword splitting)
tokens = [
    "{",
    ""na",
    "##me"",
    "##:",
    ""Priya",
    "Sha",
    "##rma",
    "##",",
    ""ed",
    "##uca",
    "##tion":",
    "[",
    "{",
    ""sc",
    "##hoo",
    "##l":",
    ""Ca",
    "##rne",
    "##gie",
    "Mellon"
    # ... more tokens
]
# Total estimated tokens: ~237
```

STEP 3: TOKEN TO INPUT IDS CONVERSION
--------------------------------------------------
```python
# ↓ VOCABULARY LOOKUP (Token → ID mapping)
input_ids = [
    21052,  # "{"
     3748,  # ""na"
      919,  # "##me""
    24399,  # "##:"
     9112,  # ""Priya"
     8124,  # "Sha"
     7414,  # "##rma"
     4672,  # "##","
    24232,  # ""ed"
     3458,  # "##uca"
    22274,  # "##tion":"
    24370,  # "["
    29334,  # "{"
    17970,  # ""sc"
     2948,  # "##hoo"
    # ... more token IDs
]
# Vocabulary size: ~30,000 tokens
# Special tokens: [CLS]=101, [SEP]=102, [PAD]=0
```

STEP 4: TOKEN EMBEDDING LOOKUP
--------------------------------------------------
```python
# ↓ EMBEDDING LOOKUP (ID → Dense Vector)
# Each token ID maps to a 768-dimensional vector
token_embeddings = [
    # Token 'Gunawan' (ID: 12847)
    [0.018099, -0.093643, -0.081261, ..., -0.053468],  # 768 dims
    # Token 'Siswo' (ID: 23691)
    [0.020404, 0.012249, 0.043204, ..., 0.040265],  # 768 dims
    # ... more token embeddings
]
# Shape: [sequence_length, 768]
```

STEP 5: POSITIONAL ENCODING
--------------------------------------------------
```python
# ↓ ADD POSITIONAL INFORMATION
# Each position gets encoded to understand word order
positional_encodings = [
    # Position 0 (first token)
    [-0.008048, -0.005079, ..., -0.022181],
    # Position 1 (second token)
    [0.036930, 0.025881, ..., -0.034034],
    # ... more positions
]

# ↓ COMBINE TOKEN + POSITIONAL EMBEDDINGS
input_embeddings = token_embeddings + positional_encodings
# Shape: [sequence_length, 768]
```

STEP 6: TRANSFORMER LAYERS PROCESSING
--------------------------------------------------
```python
# ↓ 12 TRANSFORMER LAYERS (BERT-base architecture)
hidden_states = input_embeddings

for layer_num in range(12):  # 12 transformer layers
    print(f'Processing Layer {layer_num + 1}/12')
    
    # ===== MULTI-HEAD ATTENTION =====
    # Creates Query, Key, Value matrices
    Q = hidden_states @ W_query  # [seq_len, 768] @ [768, 768]
    K = hidden_states @ W_key    # [seq_len, 768] @ [768, 768]
    V = hidden_states @ W_value  # [seq_len, 768] @ [768, 768]
    
    # Split into 12 attention heads (768/12 = 64 dims per head)
    attention_scores = []
    for head in range(12):
        q_head = Q[:, head*64:(head+1)*64]
        k_head = K[:, head*64:(head+1)*64]
        v_head = V[:, head*64:(head+1)*64]
        
        # Attention calculation: softmax(QK^T/√d_k)V
        scores = (q_head @ k_head.T) / sqrt(64)
        attention_weights = softmax(scores)
        attention_output = attention_weights @ v_head
        attention_scores.append(attention_output)
    
    # Concatenate all heads
    multi_head_output = concatenate(attention_scores)
    
    # Add & Norm (Residual connection + Layer Normalization)
    attention_output = layer_norm(hidden_states + multi_head_output)
    
    # ===== FEED FORWARD NETWORK =====
    # Two linear transformations with GELU activation
    ff_intermediate = gelu(attention_output @ W_ff1 + b_ff1)  # [seq_len, 3072]
    ff_output = ff_intermediate @ W_ff2 + b_ff2  # [seq_len, 768]
    
    # Add & Norm again
    hidden_states = layer_norm(attention_output + ff_output)
    
    # Output shape remains: [seq_len, 768]

# After 12 layers, we have contextually-aware representations
final_hidden_states = hidden_states  # [seq_len, 768]
```

STEP 7: ATTENTION MECHANISM VISUALIZATION
--------------------------------------------------
```python
# Example: How 'Machine Learning' attends to other words
# Attention weights for token 'Machine' in layer 8:
attention_example = {
    'Machine'    : 0.45,  # Self-attention (strong)
    'Learning'   : 0.32,  # Adjacent concept (strong)
    'AI'         : 0.08,  # Related field (medium)
    'Computer'   : 0.06,  # Domain context (medium)
    'Science'    : 0.04,  # Academic context (weak)
    'JavaScript' : 0.03,  # Different skill (weak)
    'Gunawan'    : 0.02,  # Name (very weak)
    # ... other tokens with smaller weights
}
# This shows the model understands 'Machine Learning' as a concept
```

STEP 8: POOLING STRATEGY (SENTENCE-LEVEL REPRESENTATION)
--------------------------------------------------
```python
# ↓ MEAN POOLING (SentenceTransformer default)
# Convert token-level representations to sentence-level

# Input: final_hidden_states shape [sequence_length, 768]
# Example with sequence_length = 150 tokens
token_representations = [
    # Token '[CLS]' representation (first 8 of 768 dims):
    [-0.015477, -0.044426, -0.056937, 0.052699, -0.079558, -0.024015, -0.028204, -0.031209, ...],
    # Token 'Gunawan' representation (first 8 of 768 dims):
    [-0.047096, -0.091310, -0.008115, -0.075035, 0.084459, -0.084240, -0.041364, 0.025728, ...],
    # Token 'Siswo' representation (first 8 of 768 dims):
    [0.077090, -0.027673, -0.061542, -0.086089, 0.032253, 0.054614, 0.097044, 0.071064, ...],
    # Token 'Machine' representation (first 8 of 768 dims):
    [0.073297, -0.023975, -0.009318, 0.066822, -0.067469, -0.028946, 0.034035, 0.040364, ...],
    # Token 'Learning' representation (first 8 of 768 dims):
    [0.036710, -0.085719, 0.026996, 0.006828, -0.051038, -0.007548, -0.046010, 0.085078, ...],
    # ... 145 more token representations
]

# ↓ APPLY MEAN POOLING
sentence_embedding = np.mean(token_representations, axis=0)
# Shape: [768] - Single vector representing entire text

# Actual result (first 10 dimensions):
sentence_embedding[:10] = [0.054892, -0.026975, 0.029953, -0.030290, 0.049536, 0.000043, 0.001778, -0.014543, 0.030940, 0.065686]
```

STEP 9: FINAL NORMALIZATION
--------------------------------------------------
```python
# ↓ L2 NORMALIZATION (Unit vector)
# Ensures all embeddings have the same magnitude

# Before normalization:
embedding_magnitude = 100.000000
embedding_vector = [0.054892, -0.026975, 0.029953, -0.030290, 0.049536, ...]

# Apply L2 normalization:
normalized_embedding = embedding_vector / ||embedding_vector||_2

# After normalization:
normalized_magnitude = 1.000000  # Always 1.0
final_embedding = [0.054892, -0.026975, 0.029953, -0.030290, 0.049536, ...]
```

STEP 10: FINAL OUTPUT - READY FOR SIMILARITY COMPARISON
--------------------------------------------------
```python
# ✅ FINAL RESULT: 768-dimensional semantic vector
user_embedding = np.array([
    # Dimensions  0- 9:
     0.054892, -0.026975,  0.029953, -0.030290,  0.049536,  0.000043,  0.001778, -0.014543,  0.030940,  0.065686,
    # Dimensions 10-19:
    -0.036186, -0.067208, -0.026283, -0.015680, -0.002136, -0.023103,  0.019992,  0.014789,  0.001283,  0.005346,
    # Dimensions 20-29:
     0.095259,  0.009505, -0.076111, -0.015217,  0.038078, -0.001985,  0.021893, -0.038337,  0.021334,  0.043269,
    # Dimensions 30-39:
     0.022870, -0.042825,  0.044482,  0.014814,  0.005506, -0.011894, -0.030185, -0.006843,  0.038574, -0.025175,
    # Dimensions 40-49:
    -0.012618,  0.037824,  0.021942,  0.019211, -0.017291, -0.049364, -0.035732, -0.024136, -0.002394, -0.029714,
    # ... dimensions 50-767
])

# Vector properties:
# - Shape: (768,)
# - Data type: <class 'numpy.ndarray'>
# - L2 norm: 1.00000000
# - Mean: 0.00012734
# - Std: 0.03608417
# - Min: -0.09977648
# - Max: 0.11496864

# This vector encodes the semantic meaning of:
# 'Gunawan Siswo Kuncoro - Front-End Developer with ML experience'
# and can now be compared with career embeddings using cosine similarity
```

STEP 11: COMPUTATIONAL COMPLEXITY ANALYSIS
--------------------------------------------------
```python
# Model Parameters and Operations:
# ================================
# BERT-base model parameters: ~110 million
# - Token embeddings: 30,000 × 768 = 23M params
# - Position embeddings: 512 × 768 = 0.4M params
# - 12 Transformer layers: ~86M params
#   * Each layer: 12 attention heads + feed-forward
#   * Attention: 4 × (768 × 768) = 2.4M params per layer
#   * Feed-forward: 768 × 3072 + 3072 × 768 = 4.7M params per layer
# 
# Forward pass operations for this text:
# - Input tokens: ~237
# - Matrix multiplications: ~11357 (Q,K,V,O for each layer)
# - Attention computations: ~8061057 operations
# - Total FLOPs: ~515907625
# 
# Processing time: ~0.150 seconds (GPU)
# Memory usage: ~0.7 MB for activations
```

================================================================================
INTERNAL MODEL PROCESSING COMPLETED
Generated: 2025-08-12T05:05:37.519536
================================================================================
